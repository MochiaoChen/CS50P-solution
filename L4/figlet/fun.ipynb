{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5876347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 193\n",
      "1: 301\n",
      "2: 300\n",
      "3: 300\n",
      "4: 300\n",
      "5: 300\n",
      "6: 300\n",
      "7: 300\n",
      "8: 300\n",
      "9: 300\n"
     ]
    }
   ],
   "source": [
    "numstr = \"\"\n",
    "for i in range(1001):\n",
    "    numstr += str(i)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{i}: {numstr.count(str(i))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965f1456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 23:57:53.320701: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 23:57:53.343704: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 23:57:53.501335: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 23:57:53.637639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753545473.754447    5563 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753545473.786153    5563 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753545474.043439    5563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753545474.043471    5563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753545474.043472    5563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753545474.043473    5563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-26 23:57:54.074514: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4c9dcc547e4296a050282fe133f8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ed14f41ccf4855ae352e0426eaf126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77ff3f6f6a34bc4a1cb5339dfce1f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93655742e0764c9fb91ec89034c74e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a201304669645e6b3540324ba124483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ÂºÄ - 0.1458\n",
      "2. ‰∏¥ - 0.0499\n",
      "3. ËßÇ - 0.0314\n",
      "4. ÊÄÄ - 0.0302\n",
      "5. Ââç - 0.0210\n",
      "6. Â∞Å - 0.0196\n",
      "7. Á™ó - 0.0179\n",
      "8. Áúã - 0.0175\n",
      "9. ÂÄö - 0.0174\n",
      "10. ÈîÄ - 0.0154\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Âä†ËΩΩÊ®°Âûã‰∏éÂàÜËØçÂô®\n",
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# ËæìÂÖ•Âè•Â≠ê\n",
    "sentence = \"Á™ó[MASK]Ë•øÂ≤≠ÂçÉÁßãÈõ™ÔºåÈó®Ê≥ä‰∏úÂê¥‰∏áÈáåËàπ\"\n",
    "masked_sentence = sentence.replace(\"[MASK]\", tokenizer.mask_token)\n",
    "\n",
    "# ÁºñÁ†Å\n",
    "inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# Ê®°ÂûãÈ¢ÑÊµã\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Ëé∑Âèñ[MASK]‰ΩçÁΩÆÈ¢ÑÊµãÁªìÊûú\n",
    "mask_logits = logits[0, mask_token_index, :]\n",
    "top_k = 10\n",
    "top_k_indices = torch.topk(mask_logits, top_k, dim=1).indices[0].tolist()\n",
    "\n",
    "# ËΩ¨‰∏∫token‰∏éÊ¶ÇÁéá\n",
    "softmax = torch.nn.functional.softmax(mask_logits, dim=1)\n",
    "top_k_probs = softmax[0, top_k_indices].tolist()\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "# ËæìÂá∫ÁªìÊûú\n",
    "for rank, (token, prob) in enumerate(zip(top_k_tokens, top_k_probs), 1):\n",
    "    print(f\"{rank}. {token} - {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11e695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ ËæìÂÖ•Âè•Â≠êÔºöÊ∞ì‰πãËö©Ëö©ÔºåÊä±Â∏ÉË¥∏‰∏ù„ÄÇÂå™Êù•Ë¥∏‰∏ùÔºåÊù•Âç≥ÊàëË∞ã„ÄÇÈÄÅÂ≠êÊ∂âÊ∑áÔºåËá≥‰∫éÈ°ø‰∏ò„ÄÇÂå™ÊàëÊÑÜÊúüÔºåÂ≠êÊó†ËâØÂ™í„ÄÇÂ∞ÜÂ≠êÊó†ÊÄíÔºåÁßã‰ª•‰∏∫Êúü„ÄÇ  ‰πòÂΩºÂûùÂû£Ôºå‰ª•ÊúõÂ§çÂÖ≥„ÄÇ‰∏çËßÅÂ§çÂÖ≥ÔºåÊ≥£Ê∂ïÊ∂üÊ∂ü„ÄÇÊó¢ËßÅÂ§çÂÖ≥ÔºåËΩΩÁ¨ëËΩΩË®Ä„ÄÇÂ∞îÂçúÂ∞îÁ≠ÆÔºå‰ΩìÊó†ÂíéË®Ä„ÄÇ‰ª•Â∞îËΩ¶Êù•Ôºå‰ª•ÊàëË¥øËøÅ„ÄÇÊ°ë‰πãÊú™ËêΩÔºåÂÖ∂Âè∂XËã•„ÄÇ‰∫éÂóüÈ∏†ÂÖÆÔºÅÊó†È£üÊ°ëËëö„ÄÇ‰∫éÂóüÂ•≥ÂÖÆÔºÅÊó†‰∏éÂ£´ËÄΩ„ÄÇÂ£´‰πãËÄΩÂÖÆÔºåÁäπÂèØËØ¥‰πü„ÄÇÂ•≥‰πãËÄΩÂÖÆÔºå‰∏çÂèØËØ¥‰πü„ÄÇ  Ê°ë‰πãËêΩÁü£ÔºåÂÖ∂ÈªÑËÄåÈô®„ÄÇËá™ÊàëÂæÇÂ∞îÔºå‰∏âÂ≤ÅÈ£üË¥´„ÄÇÊ∑áÊ∞¥Ê±§Ê±§ÔºåÊ∏êËΩ¶Â∏∑Ë£≥„ÄÇÂ•≥‰πü‰∏çÁàΩÔºåÂ£´Ë¥∞ÂÖ∂Ë°å„ÄÇÂ£´‰πüÁΩîÊûÅÔºå‰∫å‰∏âÂÖ∂Âæ∑„ÄÇ  ‰∏âÂ≤Å‰∏∫Â¶áÔºåÈù°ÂÆ§Âä≥Áü£„ÄÇÂ§ôÂÖ¥Â§úÂØêÔºåÈù°ÊúâÊúùÁü£„ÄÇË®ÄÊó¢ÈÅÇÁü£ÔºåËá≥‰∫éÊö¥Áü£„ÄÇÂÖÑÂºü‰∏çÁü•ÔºåÂí•ÂÖ∂Á¨ëÁü£„ÄÇÈùôË®ÄÊÄù‰πãÔºåË∫¨Ëá™ÊÇºÁü£„ÄÇ  ÂèäÂ∞îÂÅïËÄÅÔºåËÄÅ‰ΩøÊàëÊÄ®„ÄÇÊ∑áÂàôÊúâÂ≤∏ÔºåÈö∞ÂàôÊúâÊ≥Æ„ÄÇÊÄªËßí‰πãÂÆ¥ÔºåË®ÄÁ¨ëÊôèÊôèÔºå‰ø°Ë™ìÊó¶Êó¶Ôºå‰∏çÊÄùÂÖ∂Âèç„ÄÇÂèçÊòØ‰∏çÊÄùÔºå‰∫¶Â∑≤ÁÑâÂìâÔºÅ\n",
      "1. ‰∏ç  -  Ê¶ÇÁéá: 0.0865\n",
      "2. Ëã•  -  Ê¶ÇÁéá: 0.0570\n",
      "3. Â¶Ç  -  Ê¶ÇÁéá: 0.0562\n",
      "4. Êú™  -  Ê¶ÇÁéá: 0.0483\n",
      "5. Áäπ  -  Ê¶ÇÁéá: 0.0307\n",
      "6. Áõ∏  -  Ê¶ÇÁéá: 0.0305\n",
      "7. Ëá™  -  Ê¶ÇÁéá: 0.0274\n",
      "8. ‰∫¶  -  Ê¶ÇÁéá: 0.0231\n",
      "9. Êó†  -  Ê¶ÇÁéá: 0.0203\n",
      "10. [UNK]  -  Ê¶ÇÁéá: 0.0180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Âä†ËΩΩÊ®°ÂûãÂíåÂàÜËØçÂô®\n",
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# ‰∫§‰∫íÂºèËæìÂÖ•\n",
    "while True:\n",
    "    sentence = input(\"ËØ∑ËæìÂÖ•ÂåÖÂê´‰∏Ä‰∏™ X ÁöÑ‰∏≠ÊñáÂè•Â≠êÔºàËæìÂÖ• 'exit' ÈÄÄÂá∫ÔºâÔºö\\n> \")\n",
    "    if sentence.lower() == 'exit':\n",
    "        break\n",
    "    if \"X\" not in sentence:\n",
    "        print(\"‚ùå Âè•Â≠ê‰∏≠ÂøÖÈ°ªÂåÖÂê´‰∏Ä‰∏™ X„ÄÇËØ∑ÈáçËØï„ÄÇ\\n\")\n",
    "        continue\n",
    "\n",
    "    # ÊõøÊç¢‰∏∫Ê®°ÂûãÁöÑ mask token\n",
    "    masked_sentence = sentence.replace(\"X\", tokenizer.mask_token)\n",
    "\n",
    "    # ÁºñÁ†Å\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Ê®°ÂûãÈ¢ÑÊµã\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Ëé∑Âèñ X ‰ΩçÁΩÆÁöÑ logits Âπ∂ softmax\n",
    "    mask_logits = logits[0, mask_token_index, :]\n",
    "    softmax = torch.nn.functional.softmax(mask_logits, dim=1)\n",
    "\n",
    "    # Top 10 È¢ÑÊµã\n",
    "    top_k = 10\n",
    "    top_k_indices = torch.topk(mask_logits, top_k, dim=1).indices[0].tolist()\n",
    "    top_k_probs = softmax[0, top_k_indices].tolist()\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "    # ËæìÂá∫\n",
    "    print(f\"\\nüéØ ËæìÂÖ•Âè•Â≠êÔºö{sentence}\")\n",
    "    for i, (token, prob) in enumerate(zip(top_k_tokens, top_k_probs), 1):\n",
    "        print(f\"{i}. {token}  -  Ê¶ÇÁéá: {prob:.4f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
